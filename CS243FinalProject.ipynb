{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pcaAnalysis(df, target, features, palette='coolwarm'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform PCA analysis on a dataframe given target and features. Plots the PCA as a scatterplot\n",
    "        and shows cumulative explained variance ratio for all components.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe on which PCA is to be performed\n",
    "        target (str): Column label of target value for PCA\n",
    "        features (list[str]): List of all features from df to consider for PCA\n",
    "        palette (str): Color palette for hue used in seaborn plot\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separating out the features and target\n",
    "    x = df.loc[:, features].values\n",
    "\n",
    "    # Standardizing the features\n",
    "    x = preprocessing.scale(x, with_std=False)\n",
    "\n",
    "    #PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\n",
    "    finalDf = pd.concat([principalDf, df[[target]]], axis = 1)\n",
    "\n",
    "    #Plotting PCA visually\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x='PC1',  # Name of the first principal component in principalDf\n",
    "        y='PC2',  # Name of the second principal component in principalDf\n",
    "        hue=target,  # Color points based on the target column\n",
    "        data=finalDf,  # Use finalDf as the data source\n",
    "        palette=palette,  # Set the color palette\n",
    "        alpha=0.7  # Set transparency for better visibility\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title('PCA Result Visualization', fontsize=16)  \n",
    "    plt.xlabel('Principal Component 1', fontsize=14)  \n",
    "    plt.ylabel('Principal Component 2', fontsize=14)\n",
    "    plt.legend(title=target, bbox_to_anchor=(1.05, 1), loc='upper left')  # Customize legend\n",
    "    plt.grid(True)  # Add a grid for easier interpretation\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlapping elements\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "    # Calculate and plot cumulative explained variance ratio for all components\n",
    "    # Create new PCA object with all possible components\n",
    "    n_components = min(len(features), len(df))\n",
    "    pca_full = PCA(n_components=n_components)\n",
    "    pca_full.fit(x)\n",
    "    \n",
    "    # Calculate cumulative explained variance ratio\n",
    "    cumulative_variance_ratio = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    \n",
    "    # Create new figure for cumulative variance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), \n",
    "            cumulative_variance_ratio, \n",
    "            'bo-', \n",
    "            linewidth=2)\n",
    "    \n",
    "    # Add reference line at 95% explained variance\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Cumulative Explained Variance Ratio by Number of Components', fontsize=16)\n",
    "    plt.xlabel('Number of Components', fontsize=14)\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add percentage labels on y-axis\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def kmeansStats(df, features, max_k=25):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform k-means clustering on a dataframe, present graphs of both\n",
    "        inertial and silhouette scores for all possible k (capped at 25).\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe on which clustering is to be performed\n",
    "        features (list[str]): List of all features from df to consider for clustering\n",
    "        max_k (int): Maximum number of clusters to consider in elbow method\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value.\n",
    "    \"\"\"\n",
    "    # Separating out the features\n",
    "    x = df.loc[:, features].values\n",
    "\n",
    "    # Standardizing the features\n",
    "    x = preprocessing.scale(x, with_std=False)\n",
    "\n",
    "    # Calculate metrics for different k values\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K = range(2, max_k + 1)\n",
    "    \n",
    "    #Create model for every possible k\n",
    "    for k in K:\n",
    "        # Create and fit model\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(x)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(x, kmeans.labels_))\n",
    "\n",
    "    # Plot the curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Inertia plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(K, inertias, 'bx-')\n",
    "    plt.xlabel('k (Number of Clusters)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Silhouette score plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(K, silhouette_scores, 'rx-')\n",
    "    plt.xlabel('k (Number of Clusters)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score vs Number of Clusters')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def kmodel(df, optimal_k, features, palette='coolwarm'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform k-means clustering on a dataframe, determine optimal k using elbow method\n",
    "        and silhouette scores, and visualize results alongside PCA.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe on which clustering is to be performed\n",
    "        optimal_k (int): k to use for optimal clustering (see prior method for elbow graph)\n",
    "        features (list[str]): List of all features from df to consider for clustering\n",
    "        palette (str): Color palette for visualization\n",
    "\n",
    "    Returns:\n",
    "        KMeans: Fitted k-means model with optimal number of clusters\n",
    "    \"\"\"\n",
    "    #Generate and standardize x-values\n",
    "    x = df.loc[:, features].values\n",
    "    x = preprocessing.scale(x, with_std=False)\n",
    "\n",
    "    # Fit final model with optimal k\n",
    "    final_kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    final_kmeans.fit(x)\n",
    "    \n",
    "    # Perform PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(x)\n",
    "    \n",
    "    # Create a colormap and normalize object\n",
    "    cmap = plt.get_cmap(palette, optimal_k)  \n",
    "    norm = Normalize(vmin=0, vmax=optimal_k - 1)  # Normalize cluster labels to colormap range\n",
    "\n",
    "    # Create visualization of clusters in PCA space\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Scatter plot with explicit color assignment\n",
    "    colors = cmap(norm(final_kmeans.labels_))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], color=colors, alpha=0.7)\n",
    "\n",
    "    # Add labels to each point\n",
    "    for i, (a, b) in enumerate(zip(X_pca[:, 0], X_pca[:, 1])):\n",
    "        plt.text(a, b, f'{1974 + i}', fontsize=6, ha='right', va='bottom', color='black')\n",
    "\n",
    "    # Titles and labels\n",
    "    plt.title(f'K-means Clustering Results (k={optimal_k})\\nVisualized in PCA Space', fontsize=16)\n",
    "    plt.xlabel('First Principal Component', fontsize=14)\n",
    "    plt.ylabel('Second Principal Component', fontsize=14)\n",
    "\n",
    "    # Create a legend based on cluster labels\n",
    "    for i in range(optimal_k):\n",
    "        plt.scatter([], [], color=cmap(norm(i)), label=f'Cluster {i}')\n",
    "    plt.legend(title='Clusters', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Grid and layout\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics for each cluster\n",
    "    cluster_df = pd.DataFrame(x, columns=features)\n",
    "    cluster_df['Cluster'] = final_kmeans.labels_\n",
    "    \n",
    "    print(\"\\nCluster Sizes:\")\n",
    "    print(cluster_df['Cluster'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nCluster Centers (Standardized Features):\")\n",
    "    cluster_centers_df = pd.DataFrame(\n",
    "        final_kmeans.cluster_centers_,\n",
    "        columns=features,\n",
    "        index=[f\"Cluster {i}\" for i in range(optimal_k)]\n",
    "    )\n",
    "    print(cluster_centers_df)\n",
    "    \n",
    "    return final_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "def DBSCANStats(df, features):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform k nearest neighnors claculation on data to determine the eps value\n",
    "        to be used when performing a DBSCAN later. Plot the different distances\n",
    "        as a line graph.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe on which clustering is to be performed\n",
    "        features (list[str]): List of all features from df to consider for clustering\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value.\n",
    "    \"\"\"\n",
    "    # Separate and Standardize features\n",
    "    x = df.loc[:, features].values\n",
    "    x = preprocessing.scale(x, with_std=False)\n",
    "\n",
    "    # Compute the distances to the k-th nearest neighbor (k = min_samples - 1)\n",
    "    neighbors = NearestNeighbors(n_neighbors=5).fit(x)\n",
    "    distances, _ = neighbors.kneighbors(x)\n",
    "    distances = np.sort(distances[:, -1])  # Sort the k-distances\n",
    "\n",
    "    # Plot the k-distance graph\n",
    "    plt.plot(distances, 'bx-')\n",
    "    plt.title(\"k-distance Graph\")\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(\"k-distance\")\n",
    "    plt.show()\n",
    "\n",
    "def performDBSCAN(df, features, eps, min_samples, palette='coolwarm'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform DBSCAN clustering on a dataframe and visualize results using PCA,\n",
    "        matching the visualization style of KMeans clustering for easy comparison.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe on which clustering is to be performed\n",
    "        features (list[str]): List of all features from df to consider for clustering\n",
    "        eps (float): How close data points must be to each other to be considered part of a cluster\n",
    "        min_samples (int): Minimum number of data points required to form a distinct cluster\n",
    "        palette (str): Color palette for visualization\n",
    "\n",
    "    Returns:\n",
    "        DBSCAN: Fitted DBSCAN model\n",
    "    \"\"\"\n",
    "    # Generate and standardize x-values\n",
    "    x = df.loc[:, features].values\n",
    "    x = preprocessing.scale(x, with_std=False)\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(x)\n",
    "    \n",
    "    # Get cluster labels and number of clusters (excluding noise points labeled as -1)\n",
    "    labels = dbscan.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    # Perform PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(x)\n",
    "    \n",
    "    # Create a colormap and normalize object\n",
    "    cmap = plt.get_cmap(palette, n_clusters + 1)  # +1 for noise points\n",
    "    norm = Normalize(vmin=-1, vmax=n_clusters - 1)  # Include -1 for noise points\n",
    "    \n",
    "    # Create visualization of clusters in PCA space\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot with explicit color assignment\n",
    "    colors = cmap(norm(labels))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add labels to each point\n",
    "    for i, (a, b) in enumerate(zip(X_pca[:, 0], X_pca[:, 1])):\n",
    "        plt.text(a, b, f'{1974 + i}', fontsize=6, ha='right', va='bottom', color='black')\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f'DBSCAN Clustering Results (eps={eps}, min_samples={min_samples})\\nVisualized in PCA Space', \n",
    "              fontsize=16)\n",
    "    plt.xlabel('First Principal Component', fontsize=14)\n",
    "    plt.ylabel('Second Principal Component', fontsize=14)\n",
    "    \n",
    "    # Create a legend based on cluster labels\n",
    "    for i in range(-1, n_clusters):\n",
    "        label = 'Noise' if i == -1 else f'Cluster {i}'\n",
    "        plt.scatter([], [], color=cmap(norm(i)), label=label)\n",
    "    plt.legend(title='Clusters', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Grid and layout\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each cluster\n",
    "    cluster_df = pd.DataFrame(x, columns=features)\n",
    "    cluster_df['Cluster'] = labels\n",
    "    \n",
    "    print(\"\\nCluster Sizes:\")\n",
    "    print(cluster_df['Cluster'].value_counts().sort_index())\n",
    "    \n",
    "    # Calculate and print cluster centers (excluding noise points)\n",
    "    print(\"\\nCluster Centers (Standardized Features):\")\n",
    "    centers_list = []\n",
    "    for i in range(n_clusters):\n",
    "        mask = labels == i\n",
    "        if np.any(mask):  # Only calculate center if cluster has points\n",
    "            center = np.mean(x[mask], axis=0)\n",
    "            centers_list.append(center)\n",
    "    \n",
    "    cluster_centers_df = pd.DataFrame(\n",
    "        centers_list,\n",
    "        columns=features,\n",
    "        index=[f\"Cluster {i}\" for i in range(n_clusters)]\n",
    "    )\n",
    "    print(cluster_centers_df)\n",
    "    \n",
    "    return dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Get raw data from excel\n",
    "raw_df = pd.read_excel('historicalppi.xlsx')\n",
    "clean_df = raw_df.copy()\n",
    "\n",
    "#Process and clean the raw df\n",
    "new_header = clean_df.iloc[0]\n",
    "new_header.iloc[1:] = new_header.iloc[1:].astype(int)\n",
    "clean_df.columns = new_header\n",
    "clean_df.drop([0,17,18,19], inplace=True)\n",
    "clean_df.set_index(clean_df.columns[0], drop=True, inplace=True)\n",
    "\n",
    "'''Create ItemDF from cleanDF'''\n",
    "#Reset Index to be based off of the item and fill N/A values\n",
    "itemIndexdf = clean_df.copy()\n",
    "itemIndexdf.at['Farm-level eggs',1983] = 4.6\n",
    "itemIndexdf.at['Farm-level eggs',1984] = 12.4\n",
    "itemIndexdf.reset_index(inplace=True)\n",
    "\n",
    "'''Create yearDF from cleanDF'''\n",
    "#Set Producer Price Index item as columns rather than rows\n",
    "yearIndexdf = clean_df.copy()\n",
    "yearIndexdf = yearIndexdf.T\n",
    "yearIndexdf.reset_index(inplace=True)\n",
    "\n",
    "#Change Header Row\n",
    "header = list(clean_df.columns.copy())\n",
    "header[0] = 'Year'\n",
    "header[1] = 'Unprocessed foodstuffs and feedstuffs'\n",
    "yearIndexdf.columns = header\n",
    "\n",
    "#Enforce integer typing for year\n",
    "yearIndexdf['Year'] = yearIndexdf['Year'].astype(int)\n",
    "\n",
    "#Need to fill farm-level eggs for 1983 (6.6) & 1984 (14.4)\n",
    "#.at has to use index and header vals .iat uses only integers\n",
    "yearIndexdf.at[9,'Farm-level eggs'] = 4.6\n",
    "yearIndexdf.at[10,'Farm-level eggs'] = 12.4\n",
    "\n",
    "'''Describe the data using statistics'''\n",
    "display(yearIndexdf.describe())\n",
    "display(itemIndexdf.describe())\n",
    "\n",
    "'''Initial Graph of the data'''\n",
    "# Graphing of line plots for each item on the same graph with different colors\n",
    "plt.figure(figsize=(10, 6))  # Create a single figure\n",
    "for i in range(1, len(yearIndexdf.columns)):  \n",
    "    sns.lineplot(data=yearIndexdf, x='Year', y=yearIndexdf.columns[i], label=yearIndexdf.columns[i])\n",
    "\n",
    "plt.title(\"Features vs. Time\")\n",
    "plt.legend(title=\"Features\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PCA Analysis of both DF'''\n",
    "#PCA analysis for (Year vs Producer Price Index item) and (Producer Price Index item vs Year) respectively\n",
    "display(yearIndexdf)\n",
    "pcaAnalysis(yearIndexdf,'Year',list(yearIndexdf.columns.copy())[1:])\n",
    "display(itemIndexdf)\n",
    "pcaAnalysis(itemIndexdf,'Producer Price Index item',list(itemIndexdf.columns.copy())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''KMeans Clustering of Year Indexed DF'''\n",
    "features = list(yearIndexdf.columns.copy())[1:]\n",
    "kmeansStats(yearIndexdf,features=features)\n",
    "#Optimal should be k=4 or k=5 according to inertia and silhouette score plots\n",
    "myKMEANSmodel = kmodel(yearIndexdf, features=features, optimal_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DBSCAN Clustering of Year Indexed DF'''\n",
    "DBSCANStats(yearIndexdf,features=features) #kdistance of 80\n",
    "myDBSCANmodel = performDBSCAN(yearIndexdf,features=features,eps=80,min_samples=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
